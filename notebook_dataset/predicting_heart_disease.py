# -*- coding: utf-8 -*-
"""Predicting Heart Disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zZxiUsUORoSTKT-gJNUzJvpqFHH9bokb

# HEART DISEASE PREDICTION PROJECT

## Feature and Predictor
Our Predictor (Y, Positive or Negative diagnosis of Heart Disease) is determined by 13 features (X):

1. age (#)
2. sex : 1= Male, 0= Female (Binary)
3. (cp)chest pain type (4 values -Ordinal):Value 1: typical angina ,Value 2: atypical angina, Value 3: non-anginal pain , Value 4: asymptomatic 
4. (trestbps) resting blood pressure (#)
5. (chol) serum cholestoral in mg/dl  (#)
6. (fbs)fasting blood sugar > 120 mg/dl(Binary)(1 = true; 0 = false)
7. (restecg) resting electrocardiographic results(values 0,1,2)
8. (thalach) maximum heart rate achieved (#)
9. (exang) exercise induced angina (binary) (1 = yes; 0 = no) 
10. (oldpeak) = ST depression induced by exercise relative to rest (#) 
11. (slope) of the peak exercise ST segment (Ordinal) (Value 1: upsloping , Value 2: flat , Value 3: downsloping )
12. (ca) number of major vessels (0-3, Ordinal) colored by fluoroscopy 
13. (thal) maximum heart rate achieved - (Ordinal): 3 = normal; 6 = fixed defect; 7 = reversable defect

## Objective 
    -To predict whether a patient should be diagnosed with Heart Disease. This is a binary outcome. 
    Positive (+) = 1, patient diagnosed with Heart Disease  
    Negative (-) = 0, patient not diagnosed with Heart Disease 

    -To experiment with various Classification Models & see which yields  greatest accuracy. 
    - Examine trends & correlations within our data
    - determine which features are important in determing Positive/Negative Heart Disease

## Loading the data set
"""

import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns
import matplotlib.pyplot as plt

pip install fsspec

from google.colab import files
uploaded=files.upload()

filePath = 'heartDisease.csv'
data = pd.read_csv(filePath)
data.head(5)

"""## DATA WRANGLING"""

print("(Rows, columns): " + str(data.shape))
data.columns

data.describe()

# Display the Missing Values
print(data.isna().sum())

"""# Exploratory Data Analysis"""

#Correlation Matrix

corr = data.corr()
plt.subplots(figsize=(15,10))
sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=sns.diverging_palette(220, 20, as_cmap=True))
sns.heatmap(corr, xticklabels=corr.columns,
            yticklabels=corr.columns, 
            annot=True,
            cmap=sns.diverging_palette(220, 20, as_cmap=True))

"""A positive correlation can be seen between chest pain & target .So the more severe chest pain could result in a greater chance of having heart disease. Cp (chest pain), is a ordinal feature with 4 values: Value 1: typical angina ,Value 2: atypical angina, Value 3: non-anginal pain , Value 4: asymptomatic. 

Also we have a negative correlation between exercise induced angina & our predictor.Due to Excessive excercise heart requires more blood,but narrowed arteries slow down blood flow.

Pairplots helps to vizualize  the correlations between all variables . Here we will be plotting several features and their correlation and autocorrelation.
"""

subData = data[['age','trestbps','chol','thalach','oldpeak']]
sns.pairplot(subData)

categorical = ["target",
"sex",
"cp",
"fbs",
"restecg",
"exang",
"slope",
"ca",
"thal"]

plt.figure(figsize=(14,20))
for i in range(1,10):
    labels = data[categorical[i-1]].value_counts().index
    sizes  = data[categorical[i-1]].value_counts().values
    plt.subplot(5,2,i)
    plt.pie(sizes, labels=labels, autopct='%1.1f%%',
        shadow=True, startangle=90)
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.xticks([])
    plt.yticks([])
    plt.title(categorical[i-1].upper())
plt.show()

plt.figure(figsize=(10,17))
plt.subplot(5,2,1)
sns.kdeplot(data.loc[data["target"]==1]["age"],color="green",shade=True)
sns.kdeplot(data.loc[data["target"]==0]["age"],color="red",shade=True)
plt.legend(["target:1","target:0"])
plt.title("Age".upper())
    
for i in range(2,9):
    plt.subplot(5,2,i)
    sns.boxenplot(data=data, x=categorical[i-1],y="age")
    
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,17))
plt.subplot(5,2,1)
sns.kdeplot(data.loc[data["target"]==1]["chol"],color="green",shade=True)
sns.kdeplot(data.loc[data["target"]==0]["chol"],color="red",shade=True)
plt.legend(["target:1","target:0"])
plt.title("chol".upper())
    
for i in range(2,9):
    plt.subplot(5,2,i)
    sns.boxenplot(data=data, x=categorical[i-1],y="chol")
    
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,17))
plt.subplot(5,2,1)
sns.kdeplot(data.loc[data["target"]==1]["thalach"],color="green",shade=True)
sns.kdeplot(data.loc[data["target"]==0]["thalach"],color="red",shade=True)
plt.legend(["target:1","target:0"])
plt.title("thalach".upper())
    
for i in range(2,9):
    plt.subplot(5,2,i)
    sns.boxenplot(data=data, x=categorical[i-1],y="thalach")
    
plt.tight_layout()
plt.show()

plt.figure(figsize=(10,17))
plt.subplot(5,2,1)
sns.kdeplot(data.loc[data["target"]==1]["oldpeak"],color="green",shade=True)
sns.kdeplot(data.loc[data["target"]==0]["oldpeak"],color="red",shade=True)
plt.legend(["target:1","target:0"])
plt.title("oldpeak".upper())
    
for i in range(2,9):
    plt.subplot(5,2,i)
    sns.boxenplot(data=data, x=categorical[i-1],y="oldpeak")
    
plt.tight_layout()
plt.show()

sns.catplot(x="target", y="oldpeak", hue="slope", kind="bar", data=data);

plt.title('ST depression (induced by exercise relative to rest) vs. Heart Disease',size=25)
plt.xlabel('Heart Disease',size=20)
plt.ylabel('ST depression',size=20)

"""ST segment depression occurs because when the ventricle is at rest and therefore repolarized. If the trace in the ST segment is abnormally low below the baseline, this can lead to this Heart Disease. This is supports the plot above because low ST Depression yields people at greater risk for heart disease. While a high ST depression is considered normal & healthy. The "slope" hue, refers to the peak exercise ST segment, with values: 0: upsloping , 1: flat , 2: downsloping). Both positive & negative heart disease patients exhibit equal distributions of the 3 slope categories.

The advantages of showing the Box & Violin plots is that it showsthe basic statistics of the data, as well as its distribution. These plots are often used to compare the distribution of a given variable across some categories. 
It shows the median, IQR, & Tukeyâ€™s fence. (minimum, first quartile (Q1), median, third quartile (Q3), and maximum). In addition it can provide us with outliers in our data.
"""

plt.figure(figsize=(12,8))
sns.violinplot(x= 'target', y= 'oldpeak',hue="sex", inner='quartile',data= data )
plt.title("Thalach Level vs. Heart Disease",fontsize=20)
plt.xlabel("Heart Disease Target", fontsize=16)
plt.ylabel("Thalach Level", fontsize=16)

plt.figure(figsize=(12,8))
sns.boxplot(x= 'target', y= 'thalach',hue="sex", data=data )
plt.title("ST depression Level vs. Heart Disease", fontsize=20)
plt.xlabel("Heart Disease Target",fontsize=16)
plt.ylabel("ST depression induced by exercise relative to rest", fontsize=16)

"""# Filtering data by positive & negative Heart Disease patient"""

# Filtering data by positive Heart Disease patient 
pos_data = data[data['target']==1]
pos_data.describe()

# Filtering data by negative Heart Disease patient 
neg_data = data[data['target']==0]
neg_data.describe()

print("(Positive Patients ST depression): " + str(pos_data['oldpeak'].mean()))
print("(Negative Patients ST depression): " + str(neg_data['oldpeak'].mean()))

print("(Positive Patients thalach): " + str(pos_data['thalach'].mean()))
print("(Negative Patients thalach): " + str(neg_data['thalach'].mean()))

"""#  Machine Learning + Predictive Analytics"""

from sklearn.preprocessing import LabelEncoder

X = data.iloc[:, :-1].values
y = data.iloc[:, -1].values
encoder = LabelEncoder()
y = encoder.fit_transform(y)

"""Split: the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 42)

"""Normalize: Standardizing the data will transform the data so that its distribution will have a mean of 0 and a standard deviation of 1."""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

"""# Modeling /Training

Model 1: Logistic Regression
"""

from sklearn.metrics import classification_report 
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

model1 = LogisticRegression() # get instance of model
model1.fit(x_train, y_train) # Train/Fit model 

y_pred1 = model1.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred1)) # output accuracy


Accuracy_score= accuracy_score(y_test, model1.predict(x_test)) * 100
results_df = pd.DataFrame(data=[["Logistic Regression", Accuracy_score]], 
                          columns=['Model', 'Accuracy %'])
results_df

"""Model 2: K-NN (K-Nearest Neighbors)"""

from sklearn.metrics import classification_report 
from sklearn.neighbors import KNeighborsClassifier

model2 = KNeighborsClassifier(n_neighbors=1) # get instance of model
model2.fit(x_train, y_train) # Train/Fit model 

y_pred2 = model2.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred2)) # output accuracy

error_rate = []

# Will take some time
for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(x_train,y_train)
    pred_i = knn.predict(x_test)
    error_rate.append(np.mean(pred_i != y_test))

plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')

# NOW WITH K=16
knn = KNeighborsClassifier(n_neighbors=16)

knn.fit(x_train,y_train)
pred = knn.predict(x_test)

print('WITH K=16')
print('\n')
print(classification_report(y_test, pred)) # output accuracy

Accuracy_score= accuracy_score(y_test, knn.predict(x_test)) * 100
results_df_2 = pd.DataFrame(data=[["K-nearest neighbors", Accuracy_score]], 
                          columns=['Model', 'Accuracy %' ])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Model 3: SVM (Support Vector Machine)"""

from sklearn.metrics import classification_report 
from sklearn.svm import SVC

model3 = SVC() # get instance of model
model3.fit(x_train, y_train) # Train/Fit model

y_pred3 = model3.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred3)) # output accuracy

test_score = accuracy_score(y_test, model3.predict(x_test)) * 100

results_df_2 = pd.DataFrame(data=[["Support Vector Machine", test_score]], 
                          columns=['Model', 'Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Model 4:  Naives Bayes Classifier"""

from sklearn.metrics import classification_report 
from sklearn.naive_bayes import GaussianNB

model4 = GaussianNB() # get instance of model
model4.fit(x_train, y_train) # Train/Fit model 

y_pred4 = model4.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred4)) # output accuracy


test_score = accuracy_score(y_test, model4.predict(x_test)) * 100

results_df_2 = pd.DataFrame(data=[["Navies Bayes Classifier", test_score]], 
                          columns=['Model', 'Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Model 5: Decision Trees"""

from sklearn.metrics import classification_report 
from sklearn.tree import DecisionTreeClassifier

model5 = DecisionTreeClassifier(max_depth=3,random_state=42) # get instance of model
model5.fit(x_train, y_train) # Train/Fit model 

y_pred5 = model5.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred5)) # output accuracy


test_score = accuracy_score(y_test, model5.predict(x_test)) * 100

results_df_2 = pd.DataFrame(data=[["Decision Trees", test_score]], 
                          columns=['Model', 'Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Model 6: Random Forest"""

from sklearn.metrics import classification_report 
from sklearn.ensemble import RandomForestClassifier

model6 = RandomForestClassifier(max_depth=4,random_state=42)# get instance of model
model6.fit(x_train, y_train) # Train/Fit model 

y_pred6 = model6.predict(x_test) # get y predictions
print(classification_report(y_test, y_pred6)) # output accuracy


test_score = accuracy_score(y_test, model6.predict(x_test)) * 100

results_df_2 = pd.DataFrame(data=[["Random Forest", test_score]], 
                          columns=['Model', 'Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""Model 7:  XGBoost"""

from xgboost import XGBClassifier

model7 = XGBClassifier(max_depth=1)
model7.fit(x_train, y_train)
y_pred7 = model7.predict(x_test)
print(classification_report(y_test, y_pred7))


test_score = accuracy_score(y_test, model7.predict(x_test)) * 100

results_df_2 = pd.DataFrame(data=[["XGboost", test_score]], 
                          columns=['Model', 'Accuracy %'])
results_df = results_df.append(results_df_2, ignore_index=True)
results_df

"""# Feature Importance"""

# get importance
from sklearn.inspection import permutation_importance

imps = permutation_importance(model4, x_test, y_test)
arr=imps.importances_mean
col=data.columns[:-1]

for i in range(12):
    print("feature",col[i],"has score:     ",arr[i])

feature_score = arr

index = data.columns[:-1]

df = pd.DataFrame({'Feature Score': feature_score}, index=index)

ax = df.plot.bar()

"""From the Feature Importance graph above, we can conclude that the top 4 significant features were chest pain type (cp), maximum heart rate achieved (thalach), number of major vessels (ca), and ST depression induced by exercise relative to rest (oldpeak).

# Predictions

Scenario: A patient develops cardiac symptoms & you input his vitals into the Machine Learning Algorithm. 

He is a 20 year old male, with a chest pain value of 2 (atypical angina), with resting blood pressure of 110. 

In addition he has a serum cholestoral of 230 mg/dl. 

He is fasting blood sugar > 120 mg/dl. 

He has a resting electrocardiographic result of 1. 

The patients maximum heart rate achieved is 140.

Also, he was exercise induced angina.

His ST depression induced by exercise relative to rest value was 2.2.

The slope of the peak exercise ST segment is flat. 

He has no major vessels colored by fluoroscopy,
and in addition his maximum heart rate achieved is a reversable defect.

Based on this information, can you classify this patient with Heart Disease?
"""

print(model4.predict(sc.transform([[20,1,2,110,230,1,1,140,1,2.2,2,0,2]])))

"""# Saving the Model"""

# saving model for use
# save the model to disk
import pickle
pickle.dump(model4, open('model4', 'wb'))

